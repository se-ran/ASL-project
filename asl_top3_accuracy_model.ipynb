{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319e5ffd-e2e5-408c-b4f4-b033e433e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 16:25:35.595949: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-28 16:25:35.597066: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-28 16:25:35.648708: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-28 16:25:35.827512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-28 16:25:36.718640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f413c8e8-373f-4a74-82e7-20d44044babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open('y_train.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd58dd6e-8e0d-41fa-80a1-59e693b4c0c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_classes = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc94fd6e-65cb-46d8-ba05-d25efded395b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Classes: 2001.00\n",
      "Total Instances: 172002.00\n",
      "Mean Instances per Class: 85.96\n",
      "Median Instances per Class: 79.00\n",
      "Standard Deviation: 37.17\n",
      "Min Instances in a Class: 16.00\n",
      "Max Instances in a Class: 541.00\n",
      "25th Percentile: 61.00\n",
      "75th Percentile: 101.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "distribution = pd.DataFrame({'Class': unique, 'Count': counts})\n",
    "\n",
    "summary = distribution['Count'].describe()\n",
    "\n",
    "summary_info = {\n",
    "    'Total Classes': len(distribution),\n",
    "    'Total Instances': distribution['Count'].sum(),\n",
    "    'Mean Instances per Class': summary['mean'],\n",
    "    'Median Instances per Class': summary['50%'],\n",
    "    'Standard Deviation': summary['std'],\n",
    "    'Min Instances in a Class': summary['min'],\n",
    "    'Max Instances in a Class': summary['max'],\n",
    "    '25th Percentile': summary['25%'],\n",
    "    '75th Percentile': summary['75%']\n",
    "}\n",
    "\n",
    "for key, value in summary_info.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec4e554-81cd-43f5-b5ff-5683102e90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02569f65-f109-4a93-ac5c-9c38bffe5102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172002, 20, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e6bfee-ba24-409a-ac1c-0e24306e15f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max frames: 20\n"
     ]
    }
   ],
   "source": [
    "# Pad all videos in X_train to have the same number of frames (max_frames) with zeros\n",
    "max_frames = max(len(video) for video in X_train)\n",
    "print(\"Max frames:\", max_frames)\n",
    "\n",
    "for video_data in X_train:\n",
    "    while len(video_data) < max_frames:\n",
    "        video_data.append([0] * 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90906f6d-b9fc-45e3-b93e-c4d70f2fd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the shape and structure of each video's data in X_train\n",
    "def resolve_array(X_train):\n",
    "    n = len(X_train)  # number_of_videos\n",
    "    \n",
    "    new_X_train = []\n",
    "    \n",
    "    for i, video_data in enumerate(X_train): # Iterate through each video\n",
    "        U_X_train = np.zeros((max_frames, 34)) \n",
    "\n",
    "        for a in range(max_frames):\n",
    "            U_X_train[a][:4] = video_data[a][:4] # Copy first 4 values directly\n",
    "            for j in range(15):\n",
    "                if isinstance(video_data[a][4], (list, np.ndarray)): # If [a][4] is a list/array\n",
    "                    if np.all(np.array(video_data[a][4]) == 0): # If all values are 0\n",
    "                        U_X_train[a][4:19] = 0  \n",
    "                    else: # If not all values are 0\n",
    "                        U_X_train[a][4:19] = video_data[a][4][:15] \n",
    "                else: # If [a][4] is an integer\n",
    "                    if video_data[a][4] == 0: # If it is 0\n",
    "                        U_X_train[a][4:19] = 0 \n",
    "\n",
    "                if isinstance(video_data[a][5], (list, np.ndarray)): \n",
    "                    if np.all(np.array(video_data[a][5]) == 0): \n",
    "                        U_X_train[a][19:34] = 0\n",
    "                    else:\n",
    "                        U_X_train[a][19:34] = video_data[a][5][:15]\n",
    "                else:\n",
    "                    if video_data[a][5] == 0:\n",
    "                        U_X_train[a][19:34] = 0\n",
    "\n",
    "        new_X_train.append(U_X_train)\n",
    "    \n",
    "    return new_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c6cd40-dec6-4f8e-9dfb-2cea288dfd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = resolve_array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c229f0-b354-4023-b7ad-e2936e915f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49060359-1e62-44ae-8d01-8b6e2ef1227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "class_labels = {index: label for index, label in enumerate(label_encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c38203b-8d7e-43f1-921a-e82d6c118acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.astype('float32')\n",
    "y_encoded = y_encoded.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d9d398-0b95-4901-b3a3-dc74ed63b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172002\n",
      "172002\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971679ef-13b3-4a05-871b-c58756a2affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (172002, 20, 34)\n",
      "Resampled dataset shape: (1082541, 20, 34)\n",
      "Original class distribution: [ 64  56  59 ...  59  80 541]\n",
      "Resampled class distribution: [541 541 541 ... 541 541 541]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 1. Calculate the number of samples per class\n",
    "class_counts = np.bincount(y_encoded)\n",
    "\n",
    "\n",
    "# 2. Duplicate samples for classes with only one sample to ensure minimum count of 2\n",
    "for i, count in enumerate(class_counts):\n",
    "    if count == 1:\n",
    "        index = np.where(y_encoded == i)[0][0]\n",
    "        X = np.concatenate([X, X[index:index+1]], axis=0)\n",
    "        y_encoded = np.concatenate([y_encoded, y_encoded[index:index+1]], axis=0)\n",
    "\n",
    "# 3. Reshape data to 2D for SMOTE application\n",
    "n_samples, max_frames, M = X.shape\n",
    "X_reshaped = X.reshape(n_samples, -1)\n",
    "\n",
    "# 4. Apply SMOTE\n",
    "min_class_samples = class_counts[class_counts > 0].min()\n",
    "k_neighbors = max(min(min_class_samples - 1, 3), 1)\n",
    "smote = SMOTE(sampling_strategy='auto', k_neighbors=k_neighbors, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_reshaped, y_encoded)\n",
    "\n",
    "# 5. Reshape the data back to 3D\n",
    "X_resampled = X_resampled.reshape(-1, max_frames, M)\n",
    "\n",
    "print(f\"Original dataset shape: {X.shape}\")\n",
    "print(f\"Resampled dataset shape: {X_resampled.shape}\")\n",
    "print(f\"Original class distribution: {np.bincount(y_encoded)}\")\n",
    "print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc58d99c-694b-4ec9-95d7-f7b9e5c64216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 16:40:17.858749: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,337,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2001</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">514,257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m4,337,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,049,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2001\u001b[0m)           │       \u001b[38;5;34m514,257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,229,713</span> (23.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,229,713\u001b[0m (23.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,229,713</span> (23.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,229,713\u001b[0m (23.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the number of sign language gestures to recognize\n",
    "N = 2001\n",
    "\n",
    "# Define the number of input features computed in one frame\n",
    "M = 34 \n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_frames, M), name='input'), \n",
    "    tf.keras.layers.LSTM(1024, return_sequences=False), # false로 바꿈 / 512에서 1024로 \n",
    "    tf.keras.layers.Dense(1024, activation=tf.nn.relu), \n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu), \n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(N, activation=tf.nn.softmax, name='output')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52fb5fb-44b0-4450-b92b-513824605e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 1. One-hot encode y_resampled\n",
    "num_classes = len(np.unique(y_resampled))\n",
    "y_resampled_one_hot = to_categorical(y_resampled, num_classes=num_classes)\n",
    "\n",
    "# 2. Normalize X_resampled\n",
    "num_samples, num_frames, num_features = X_resampled.shape\n",
    "scaler = MinMaxScaler()\n",
    "X_resampled_reshaped = X_resampled.reshape(-1, num_features)  # Reshape to (samples * frames, features)\n",
    "X_resampled_normalized = scaler.fit_transform(X_resampled_reshaped)\n",
    "X_resampled_normalized = X_resampled_normalized.reshape(num_samples, num_frames, num_features)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled_normalized, y_resampled_one_hot, \n",
    "                                                  test_size=0.2, random_state=42)\n",
    "\n",
    "# Top-3 Accuracy \n",
    "top_k_metric = tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='CategoricalCrossentropy', \n",
    "              metrics=['accuracy', top_k_metric])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=30, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "print(\"Final Training Accuracy: \", history.history['accuracy'][-1])\n",
    "print(\"Final Training Top-3 Accuracy: \", history.history['top_3_accuracy'][-1])\n",
    "print(\"Final Validation Accuracy: \", history.history['val_accuracy'][-1])\n",
    "print(\"Final Validation Top-3 Accuracy: \", history.history['val_top_3_accuracy'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e227be-c0af-4981-b48d-c9c3319b3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('asl_top3_accuracy_model.h5')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aslenv",
   "language": "python",
   "name": "aslenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
