{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 18:17:29.145874: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 18:17:29.146306: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-28 18:17:29.149499: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-28 18:17:29.190045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 18:17:29.859195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "\n",
    "video_dir = 'videos/'\n",
    "\n",
    "\n",
    "# with open('processed_files.pkl', 'wb') as f:\n",
    "#     pickle.dump(processed_files_data, f)\n",
    "\n",
    "# with open('X_train.pkl', 'wb') as f: \n",
    "#     pickle.dump(X_train_data, f)\n",
    "\n",
    "# with open('y_train.pkl', 'wb') as f:\n",
    "#     pickle.dump(y_train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pose_landmarks(results, frame_number):\n",
    "    if results.pose_landmarks is None:\n",
    "        print(f\"Frame {frame_number}: No pose landmarks detected\")\n",
    "        return None\n",
    "\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "    \n",
    "    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y] #11\n",
    "    left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y] #13\n",
    "    left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y] #15\n",
    "\n",
    "    right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y] #12\n",
    "    right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y] #14\n",
    "    right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y] #16\n",
    "\n",
    "    left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y] #23\n",
    "    right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y] #24\n",
    "\n",
    "    if left_hip[0] >= 0 and right_hip[0] >= 0:\n",
    "        middle_hip = [(left_hip[0] - right_hip[0]) / 2, (left_hip[1] - right_hip[1]) / 2]\n",
    "    else:\n",
    "        middle_hip = None\n",
    "        print(\"Hip coordinates are negative\")\n",
    "\n",
    "    angle_left_arm = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "    angle_right_arm = calculate_angle(right_wrist, right_elbow, right_shoulder)\n",
    "    angle_right_arm_to_hip = calculate_angle(right_elbow, right_shoulder, middle_hip)\n",
    "    angle_left_arm_to_hip = calculate_angle(middle_hip, left_shoulder, left_elbow)\n",
    "\n",
    "    frame_results = {\n",
    "        \"Left Arm\": round(angle_left_arm, 3),\n",
    "        \"Right Arm\": round(angle_right_arm, 3),\n",
    "        \"Right Arm to Hip\": round(angle_right_arm_to_hip, 3),\n",
    "        \"Left Arm to Hip\": round(angle_left_arm_to_hip, 3)\n",
    "    }\n",
    "\n",
    "    print(f\"Frame {frame_number+1} pose:\", frame_results)\n",
    "    \n",
    "    return frame_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_left_hand_landmarks(left_hand_landmarks, frame_number):\n",
    "    if not left_hand_landmarks:\n",
    "        print(f\"Frame {frame_number}: No left hand landmarks detected\")\n",
    "        return None  \n",
    "\n",
    "    joint = np.zeros((21, 3))\n",
    "    for i, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "        joint[i] = [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "    v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :]\n",
    "    v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :]\n",
    "    v = v2 - v1\n",
    "\n",
    "    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "    compareL1 = v[[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17], :]\n",
    "    compareL2 = v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :]\n",
    "\n",
    "    L_angle = np.arccos(np.einsum('nt,nt->n', compareL1, compareL2))\n",
    "    L_angle = np.degrees(L_angle)\n",
    "\n",
    "    frame_results = {\"Left Hand\": L_angle}\n",
    "    print(f\"Frame {frame_number + 1} left hand:\", frame_results)\n",
    "\n",
    "    return frame_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_right_hand_landmarks(right_hand_landmarks, frame_number):\n",
    "    if not right_hand_landmarks:\n",
    "        print(f\"Frame {frame_number}: No right hand landmarks detected\")\n",
    "        return None\n",
    "\n",
    "    joint = np.zeros((21, 3))\n",
    "    for i, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "        joint[i] = [landmark.x, landmark.y, landmark.z]\n",
    "\n",
    "    v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :]\n",
    "    v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :]\n",
    "    v = v2 - v1\n",
    "\n",
    "    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "    compareR1 = v[[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17], :]\n",
    "    compareR2 = v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :]\n",
    "\n",
    "    R_angle = np.arccos(np.einsum('nt,nt->n', compareR1, compareR2))\n",
    "    R_angle = np.degrees(R_angle)\n",
    "\n",
    "\n",
    "    frame_results = {\"Right Hand\": R_angle}\n",
    "    print(f\"Frame {frame_number + 1} right hand:\", frame_results)\n",
    "\n",
    "    return frame_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frame_properties(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, frame = cap.read()\n",
    "    height, width, channels = frame.shape\n",
    "    dtype = frame.dtype  \n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return height, width, channels, dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frames starting at start_frame for frame_count frames\n",
    "def extract_frames(video_path, start_frame, frame_count, frame_interval):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    height, width, channels, dtype = get_video_frame_properties(video_path)\n",
    "    \n",
    "    for i in range(frame_count):\n",
    "        frame_number = start_frame + i * frame_interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(f\"Cannot read frame {frame_number}.\")\n",
    "            break\n",
    "        \n",
    "        frames.append(image)\n",
    "        \n",
    "    cap.release()\n",
    "    \n",
    "    while len(frames) < frame_count:\n",
    "        frames.append(np.zeros((height, width, channels), dtype=np.uint8))\n",
    "    \n",
    "    \n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_continuous_frames(video_path, label, frame_interval=5, frame_count=20): \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    sequences = []\n",
    "    \n",
    "    if total_frames >= frame_count:\n",
    "        print(f\"{video_path} (Label: {label}): Total Frame: {total_frames}\")\n",
    "        for start_frame in range(0, total_frames - frame_count + 1, frame_interval):\n",
    "            frames = extract_frames(video_path, start_frame, frame_count, frame_interval)\n",
    "            if len(frames) == frame_count:\n",
    "                sequences.append(frames)        \n",
    "            else:\n",
    "                print(f\"Cannot extract {frame_count} continuous frames starting at frame {start_frame} from {video_path}.\")   \n",
    "        print(f\"Extracted {len(sequences)} sequences of {frame_count} frames from {video_path}. - moving sample\")\n",
    "    else:\n",
    "        # If there are not enough frames, extract once and pad\n",
    "        print(f\"{video_path} (Label: {label}): Not enough frames ({total_frames} frames, less than {frame_count}), generating a single sample.\")\n",
    "        frames = extract_frames(video_path, 0, frame_count, 1)\n",
    "        if len(frames) < frame_count:  # Padding\n",
    "            frames.extend([None] * (frame_count - len(frames)))\n",
    "        sequences.append(frames)\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the shape and structure of each video's data in X_train\n",
    "def resolve_array(X_train):\n",
    "    n = len(X_train)  # number_of_videos\n",
    "    \n",
    "    new_X_train = []\n",
    "    \n",
    "    for i, video_data in enumerate(X_train): # Iterate through each video\n",
    "        U_X_train = np.zeros((max_frames, 34)) \n",
    "\n",
    "        for a in range(max_frames):\n",
    "            U_X_train[a][:4] = video_data[a][:4] # Copy first 4 values directly\n",
    "            for j in range(15):\n",
    "                if isinstance(video_data[a][4], (list, np.ndarray)): # If [a][4] is a list/array\n",
    "                    if np.all(np.array(video_data[a][4]) == 0): # If all values are 0\n",
    "                        U_X_train[a][4:19] = 0  \n",
    "                    else: # If not all values are 0\n",
    "                        U_X_train[a][4:19] = video_data[a][4][:15] \n",
    "                else: # If [a][4] is an integer\n",
    "                    if video_data[a][4] == 0: # If it is 0\n",
    "                        U_X_train[a][4:19] = 0 \n",
    "\n",
    "                if isinstance(video_data[a][5], (list, np.ndarray)): \n",
    "                    if np.all(np.array(video_data[a][5]) == 0): \n",
    "                        U_X_train[a][19:34] = 0\n",
    "                    else:\n",
    "                        U_X_train[a][19:34] = video_data[a][5][:15]\n",
    "                else:\n",
    "                    if video_data[a][5] == 0:\n",
    "                        U_X_train[a][19:34] = 0\n",
    "\n",
    "        new_X_train.append(U_X_train)\n",
    "    \n",
    "    return new_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, target_length=20):\n",
    "    while len(sequence) < target_length:\n",
    "        sequence.append([\n",
    "            0, 0, 0, 0,  # Pose\n",
    "            [0] * 15,    # Left Hand\n",
    "            [0] * 15     # Right Hand\n",
    "        ])\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file = 'WLASL_v0.3.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "video_label_map = {}\n",
    "for item in data:\n",
    "    gloss = item[\"gloss\"]\n",
    "    instances = item[\"instances\"]\n",
    "    for instance in instances:\n",
    "        video_id = instance[\"video_id\"]\n",
    "        if video_id not in video_label_map:\n",
    "            video_label_map[video_id] = gloss\n",
    "y = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_pickle(file_path, default_value):\n",
    "    \"\"\"Attempts to load pickle data. If corrupted, returns default value.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except (pickle.UnpicklingError, EOFError) as e:\n",
    "        print(f\"Error: {file_path}, {e}\")\n",
    "        return default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_pickle(file_path, default):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(file_path, data):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Initialization\n",
    "processed_files = safe_load_pickle('processed_files.pkl', set())\n",
    "X_train = safe_load_pickle('X_train.pkl', [])\n",
    "y = safe_load_pickle('y_train.pkl', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frames -> Sequences\n",
    "\n",
    "processed_count = 0\n",
    "SAVE_INTERVAL = 10\n",
    "\n",
    "for filename in os.listdir(video_dir):\n",
    "    if filename in processed_files:\n",
    "        print(f\"{filename} has already been processed.\")\n",
    "        continue\n",
    "\n",
    "    video_path = os.path.join(video_dir, filename)\n",
    "    video_id = filename.split('.')[0]\n",
    "    label = video_label_map.get(video_id)\n",
    "    frame_sequences = extract_continuous_frames(video_path, label)\n",
    "    total_frames = int(cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FRAME_COUNT)) # Added for logging purposes\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video: {video_path}\")\n",
    "        continue \n",
    "\n",
    "\n",
    "    all_sequences = []  # Store the final sequences\n",
    "    prev_pose_results = None\n",
    "    prev_left_hand_results = None\n",
    "    prev_right_hand_results = None\n",
    "    hand_detected = False  # Variable to track when hands are first detected\n",
    "    \n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.8, min_tracking_confidence=0.8) as holistic:\n",
    "        frame_number = 0\n",
    "        for frames in frame_sequences:\n",
    "            sequence_angles = []  # List to store data for each sequence\n",
    "            for image in frames:\n",
    "                if image is None:\n",
    "                    print(f\"No frame available: {video_path}, Frame number: {frame_number}\")\n",
    "                    continue\n",
    "\n",
    "                image.flags.writeable = False\n",
    "                results = holistic.process(image)\n",
    "                image.flags.writeable = True\n",
    "\n",
    "                pose_results = process_pose_landmarks(results, frame_number)\n",
    "                left_hand_results = process_left_hand_landmarks(results.left_hand_landmarks, frame_number)\n",
    "                right_hand_results = process_right_hand_landmarks(results.right_hand_landmarks, frame_number)\n",
    "\n",
    "                # Check if hands are detected\n",
    "                if (left_hand_results or right_hand_results) and not hand_detected:\n",
    "                    hand_detected = True\n",
    "                    print(f\"Hands first detected: {filename}\")\n",
    "\n",
    "                # Only create sequences after hands are detected\n",
    "                if hand_detected:\n",
    "                    if not (pose_results or left_hand_results or right_hand_results):\n",
    "                        # If pose, left hand, and right hand are all undetected, exclude the frame\n",
    "                        print(\"Unable to detect any angles: excluding frame\")\n",
    "                        frame_number += 1\n",
    "                        continue  \n",
    "\n",
    "                    # Use previous frame's angles if current frame angles are undetected\n",
    "                    pose_results = (\n",
    "                        pose_results\n",
    "                        if pose_results is not None\n",
    "                        else prev_pose_results or {\"Left Arm\": 0, \"Right Arm\": 0, \"Right Arm to Hip\": 0, \"Left Arm to Hip\": 0}\n",
    "                    )\n",
    "\n",
    "                    left_hand_results = (\n",
    "                        left_hand_results\n",
    "                        if left_hand_results is not None\n",
    "                        else prev_left_hand_results or {\"Left Hand\": [0] * 15}\n",
    "                    )\n",
    "\n",
    "                    right_hand_results = (\n",
    "                        right_hand_results\n",
    "                        if right_hand_results is not None\n",
    "                        else prev_right_hand_results or {\"Right Hand\": [0] * 15}\n",
    "                    )\n",
    "\n",
    "                    frame_data = [\n",
    "                        pose_results[\"Left Arm\"], pose_results[\"Right Arm\"],\n",
    "                        pose_results[\"Right Arm to Hip\"], pose_results[\"Left Arm to Hip\"],\n",
    "                        left_hand_results[\"Left Hand\"], right_hand_results[\"Right Hand\"]\n",
    "                    ]\n",
    "\n",
    "                    sequence_angles.append(frame_data)\n",
    "                    \n",
    "                    \n",
    "                frame_number += 1\n",
    "                prev_pose_results = pose_results\n",
    "                prev_left_hand_results = left_hand_results\n",
    "                prev_right_hand_results = right_hand_results\n",
    "\n",
    "            # Add sequence if complete; pad if insufficient\n",
    "            if len(sequence_angles) > 0:\n",
    "                if len(sequence_angles) < 20:\n",
    "                    sequence_angles = pad_sequence(sequence_angles)\n",
    "                all_sequences.append(sequence_angles)                \n",
    "                \n",
    "\n",
    "        # Add all sequences to X_train and y\n",
    "        if len(all_sequences) > 0:\n",
    "            X_train.extend(all_sequences)  # Add sequences\n",
    "            y.extend([label] * len(all_sequences))  # Map labels to sequences\n",
    "            print(f\"{filename}: {len(all_sequences)} sequences added.\")\n",
    "        else:\n",
    "            print(f\"Not enough frame data from {filename}.\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    processed_files.add(filename)\n",
    "    processed_count += 1\n",
    "\n",
    "    if processed_count % SAVE_INTERVAL == 0:\n",
    "        save_pickle('processed_files.pkl', processed_files)\n",
    "        save_pickle('X_train.pkl', X_train)\n",
    "        save_pickle('y_train.pkl', y)\n",
    "\n",
    "# Final data save\n",
    "save_pickle('processed_files.pkl', processed_files)\n",
    "save_pickle('X_train.pkl', X_train)\n",
    "save_pickle('y_train.pkl', y)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aslenv",
   "language": "python",
   "name": "aslenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
